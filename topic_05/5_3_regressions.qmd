---
title: "Regression, pt. 1: Running the model"
---

Please note that this walkthrough assumes that you have the following packages installed and loaded in R:

```{r}
pacman::p_load(car, # qqPlot function
               cowplot, # apa plotting
               tidyverse, # tidyverse goodness
               psych, # descriptive stats
               lm.beta, # getting a standard coefficient
               broom, # tidy lm output
               olsrr # testing heteroscadicity.
               ) 
```

## Fitting the data to a model

As social scientists, we are not only concerned with observation, but also with explanation. Measures of correlation provide us with the former; we use regression methods to build models in service of the latter. Again both the assigned texts provide excellent overviews of regression, so I won't repeat much of what they say here.

For the purposes of this vignette (and this course) we will limit ourselves to linear regression, that is describing a line that best fits our data. By "fit" we mean a line than when compared to our observed data minimizes our squared residuals.

## Modeling the Relationship b/tw Stress and Health

Let's revisit our data from the first walk-through:

> Wagner, Compas, and Howell (1988) investigated the relationship between stress and mental health in first-year college students. Using a scale they developed to measure the frequency, perceived importance, and desirability of recent life events, they created a measure of negative events weighted by the reported frequency and the respondent's subjective estimate of the impact of each event. This served as their measure of the subject's perceived social and environmental stress. They also asked students to complete the Hopkins Symptom Checklist, assessing the presence or absence of 57 psychological symptoms.

This data can be accessed directly from this [companion website](https://www.uvm.edu/~statdhtx/methods8/) for Howell's stats textbook.

```{r}
stress_data <- read_table("https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab9-2.dat")

psych::describe(stress_data)
```

To perform a linear regression we may call upon the `lm()` function. This function uses formula notation `outcome variable ~ predictor variable(s)`. A simple regression has a single predictor. More often in our analyses we are concerned with the relative effects of multiple predictors, but this is multiple regression and saved for the Spring course. Here, we may be interested in the degree to which perceived **Stress** contributes to the number of **lnSymptons**, or using formula terminology: "How do lnSymptoms (outcome) vary as a function of Stress (predictor)". This is represented in R as `stress_data$lnSymptoms~stress_data$Stress`

## Linear models in R

Let's run the model we introduced in the last section. Note that instead of specifying the data frame for each variable, I can place it in the `data=` argument.

```{r warning = FALSE}
lm(lnSymptoms~Stress,data = stress_data)
```

This output gives us our `intercept` and `slope` coefficients. That said, theirs more lurking behind this output. The resulting output of the `lm()` function is an `object` with the class `lm`. This simply means that R understands that this object is storing a linear model. Hiding behind this spartan output is a multitude of info that may be accessed via its `attributes` or it may be thrown into other functions for additional info and analysis. **My tip... GET IN THE HABIT OF SAVING (ASSIGNING) YOUR MODELS**. In this case let's assign the model we just ran to `stress_symptoms_model`

```{r}
stress_symptoms_model <- lm(lnSymptoms~Stress,data = stress_data)
```

### Attributes of class `lm`:

As I mentioned above, `lm` objects contain various attributes that may be accessed using the names convention, `$nameOfAttribute`.

For example, returning to the `stress_symptoms_model` object we created above, we may get a glimpse of its attributes by:

```{r warning = FALSE}
attributes(stress_symptoms_model)
stress_symptoms_model$residuals
```

This command tells us the names of several attributes (values) that are stored in `stress_symptoms_model`. Typically we are interested in the `$coefficients`, `fitted.values` (aka *predicted values*), and `$residuals`. Recall that in our model `fitted.values` are the predicted values of **lnSymptoms** for each value of **Stress**, that is those values that fall along the line of best fit. `residuals` are the difference between our observed values of **lnSymptoms** and those predicted by the model. Mathematically `stress_symptoms_model$residuals` is equivalent to:

```{r eval=FALSE}
stress_data$lnSymptoms - stress_symptoms_model$fitted.values
```

### Testing the residuals

An important assumption test for our model is that the residuals are normally distributed. In fact some argue that this is more important than than having normal distributions in the raw data itself. Quickly, we can test this using the regular methods:

```{r eval=FALSE}
hist(stress_symptoms_model$residuals,breaks = 10)
car::qqPlot(stress_symptoms_model$residuals) %>% show()
psych::describe(stress_symptoms_model$residuals)
```

**Question: Run a Shapiro-Wilkes test on our `stress_symptoms_model` residuals. How do we feel about the residuals here?**

```{r}
# test the stress_symptoms_model residulas

```

We'll return to diagnosing your regression model for violations of your assumptions (normality & heteroscadicity) in an an upcoming walkthrough. For now, I just wanted to use this as an example of extracting values from your model.

### Using your `lm` object with other functions

As I mentioned above you may also use `lm` objects with other functions. For example, for our model `stress_symptoms_model`:

```{r eval=TRUE}
summary(stress_symptoms_model)
```

provides us with info about the coefficients, their standard error, *t*-scores, and corresponding p-values (Pr\>\|t\|). It also provides us with our $r^2$ and adjusted $r_{adj}^2$ (which corresponds to our $r$ and $r_{adj}$ from above).

Coincidentally we can also call the individual attributes of this summary as well:

```{r warning = FALSE}
summary(stress_symptoms_model) %>% attributes()
summary(stress_symptoms_model)$coefficients
```

For what it's worth, Bodo Winter (optional text) is a fan of using the `tidy` function from the `broom` package to clean up the output. I prefer `summary` but can understand the desire for clean and to the point output.

```{r}
pacman::p_load(broom)
broom::tidy(stress_symptoms_model)
```

## A note on degrees of freedom of our model

The general equation for calculating the requisite degrees of freedom of a model is $n-k$ where $n$ is the number of observations (or later conditions) and $k$ is the number of parameters that are required to be known for the calculation.

Recall how we calculate variance / standard deviation. We take the sum of the squared deviations from the *mean* $\sum(X-\bar{X})$ and divide by $(n-1)$. Remember that we are subtracting that 1 because in order to calculate the variance we must keep one thing constant, the **mean**. In this case the **mean** is our parameter that must be known in order to calculate the variance.

We can now ask the question "what must be known in order to derive the line of best fit?", or more simply what must one know in order to create a particular line. We must know **both** the line's **slope** and its **intercept**. Knowing slope alone leads to the indeterminism presented in Fig A (identical slopes, infinitely-many intercepts), intercept alone leads to indeterminism presented in Fig B (identical intercept, infinitely-many slopes). We need **both** to define a particular line. Just like the **mean** above, these are the two parameters that must be *locked-in* to derive our model.

```{r echo=FALSE, fig.height=4, fig.width=8}
figA <- ggplot() +
  xlab("y-intercept")+
  xlim(c(-10,10))+ylim(c(-10,10))+
  geom_vline(xintercept = 0, linetype="dashed", color = "black", size=1) +
  geom_abline(intercept=c(0,0), slope=1) +
  geom_abline(intercept=c(0,3), slope=1) +
  geom_abline(intercept=c(0,6), slope=1) +
  geom_abline(intercept=c(0,9), slope=1) +
  geom_abline(intercept=c(0,-3), slope=1) +
  geom_abline(intercept=c(0,-6), slope=1) +
  geom_abline(intercept=c(0,-9), slope=1)


figB <- ggplot() +
  xlab("y-intercept")+
  xlim(c(-10,10))+ylim(c(-10,10))+
  geom_vline(xintercept = 0, linetype="dashed", color = "black", size=1) +
  geom_abline(intercept=c(0,0), slope=-1.5) +
  geom_abline(intercept=c(0,0), slope=-1) +
  geom_abline(intercept=c(0,0), slope=-.5) +
  geom_abline(intercept=c(0,0), slope=0) +
  geom_abline(intercept=c(0,0), slope=.5) +
  geom_abline(intercept=c(0,0), slope=1) +
  geom_abline(intercept=c(0,0), slope=1.5)

cowplot::plot_grid(figA,figB, labels = c("A", "B"))
```

A simple way of restating this is that your model degrees of freedom are the: $$number \space of \space observations -  number \space of \space predictors$$

where at least one of your predictors is used as a constant (the intercept) and the remainder provide your $\beta$ coefficients.

For simple regression we have 1 predictor and 1 constant so our degrees of freedom are $n-k$; where $k=2$. However, in your future **there be be dragons**... models that may have multiple predictors and multiple intercepts (multiple regression, mixed effects / multi-level models, etc). Indeed, using `df` may not even make much sense in such cases, and there are arguments about how to even calculate them in the first place.

## Interpreting the output

With our plot in hand, we can return to our `summary(stress_symptoms_model)` output:

```{r}
summary(stress_symptoms_model)
```

-   **Call**: tells us the formula we originally input for this model

-   **Residuals**: descriptive stats of the residuals

-   **Coefficients**: provides the estimated values, standard error of the estimates, and NHST of the intercept and beta coefficient. In both cases the null hypothesis is Estimate = 0. Again for the intercept this may or may not be useful. For example you might use it to test whether people who haven't had a single drink of alcohol have no depression at all. The test of `beta` tests against the null hypothesis of zero correlation (relationship). All tests are conducted provided the t-value on $n-k$ degrees of freedom (see next bullet point)

-   **Residual standard error**: This is calculated by first getting the sum of the squared residuals and dividing that number by the degrees of freedom of the model. In this case the df equals the number of total observations (107) minus the number of parameters ($k$) that were calculated by the model in order to generate the predicted values (2). This may be interpreted as a measure of goodness of fit of the model, and perhaps more importantly its predictive value. This value is closely tied to Cohen Chapter 10 "The Variance of the Estimate", but rather than the denominator being $N$, it is the degrees of freedom for the model. In this way it speaks more directly to the model itself.

-   **Multiple R-squared** and **Adjusted R-squared**: The **coefficient of determination** calculated from $r$ or $r_{adj}$. $r^2$ tells us the degree to which our model accounts for observed variance in the observed outcomes. On was to think about this is, to what degree does variation in our predictor explain the observed variation in our outcomes. More on this below.

-   Finally, this output gives is the resulting $F$-ratio and corresponding $df$ for an $F$-test. This is a significance test for our $r^2$. Does our model explain a significant amount of the variance?

## The coeff. of determination as index of explanatory value

Here we may turn to a (a little bit longer than expected) digression involving J.S. Mill and *exact* and *inexact* sciences. One philosophical conceit is that we can never really empirically observe causes, but only effects. That is, I can never truly observe that one billiard ball *caused* another to move, I can only say with a high degree of certainty that I observed the first billiard ball striking the second and subsequently the second moved at a particular velocity. In order to deal with this epistemological crisis (how can you really ever know anything!!!) we build explanatory models. The degree to which our model is able to account for the range of our previous observations tells us how useful our model parameters (predictors) are in explaining a phenomenon in question.

For example, to explain what I observe about the contact and subsequent motion of billiard balls I might build a model that takes into account the mass and velocity (speed and direction) of the first ball, the mass and velocity of the second ball and the point and angle of contact. Presuming I use these as predictors, I can account with varying degrees of success the heading and direction of the balls, or the *outcome*. In fact, in this example, I'm likely to be very successful in describing most, **but not all** observed outcomes. That is, there are some variations in the outcomes that just using these parameters does not result in a successful account. I can refine my *billiard contact model* to include things like the friction caused from the felt, the force of gravity, the angle of the table top, etc., and it's likely the case that with each new predictor my model becomes better. However at some point I begin to reach diminishing returns (see over-fitting next semester).

This is why physics *appears* to be an exact science (well, mechanical physics at least). They've had centuries to develop and refine their models about the mechanical workings of our very local corner of the universe to the point that these models are able to account for a very high degree of variation in observed phenomena. These models typically have a very high $r^2$ and as such tend to have high predictive value. Which is why with some understanding of gravitational force, mass, and a few simple calculations, high school students all over country are able to do [this](https://www.youtube.com/watch?v=46CJmWlxfck) in their physics lab.

Indeed within some disciplines having an $r^2$ \< .9 means you model isn't good enough. Typically this is the case when the phenomenon under observation is not complex (hope that doesn't offend). As you move away from physics and chemistry (or at least the low complexity phenomena under each discipline), the explanatory value of models tends to decrease. Us folks in the social sciences, we are moving into inexact-science-land. Weather patterns, economies, ecosystems, and human behaviors are highly complex; in many cases in inordinate number of contributing factors may be present in an observed outcome. As a result degree of variation that these models may account for is typically diminished, especially in the social sciences. For example depending on the phenomena and sub-discipline, an $r^2$ of .3 might mean you're cooking with gas! In fact, for some phenomena, a reported $r^2$ that is too high might be a cause for suspicion ("they weren't so much cooking with gas so much as they were cooking the books").

As a final note, its typically poor practice to get a significant $r^2$ and just stop there. Remember, this is just a claim that your model accounts for significantly more variation than 0%. If you were to have a model that accounted for 6% ($r^2 = .06$) of the variance but was significant ($p<.05$), you need to ask yourself whether that model is useful at all. It certainly has very little predictive value (I wouldn't bet my life... or even a few bills on it predicting a future outcome), and its true explanatory value is quite questionable.

![](https://imgs.xkcd.com/comics/linear_regression.png)

For what it's worth, this isn't just a joke. I can point to a paper from my field that was published in a very prominent journal (\*\*cough Psych Science) that reported just this... a $r^2$ of about .08.

## Write up and presentation

Our obtained results can be reported in several different ways. However, typically it's best practice to report both the $beta$ ($b$) coefficient with corresponding $t-test$; as well as the general statistics about the model including the $r^2$ and corresponding $F-test$. Prior to any reporting of the stats, you also need to articulate the structure of the model.

I think a good template to start from is:

> "\[Clearly restate the alternative hypothesis identifying operationalized variables\]. To test this hypothesis the data were \[what type of analysis?\] with \[what is your dv\] as the outcome and \[what is your predictor\] as our predictor. The resulting model was significant, $r^2$=0.XX; $F$(df1,df2)=F-ratio, $p$=p-value and revealed a significant relationship between the \[covariables\], $b$=XX; $t$(df)=XX, $p$=XX\$."

Returning to our `stress_symptoms_model`:

```{r}
summary(stress_symptoms_model)
```

For example, with this data you may report it as:

> "We hypothesized that increases in self-reported measures of Stress would correspond to increases in the number of self-reported symptoms. Given that our Symptoms data showed a significant violation of the normality assumption, we transformed these scores using a log-transformation (lnSymptoms). To test this hypothesis the data were submitted to a simple linear regression with lnSymptoms as the outcome and Stress as our predictor. The resulting model was significant, $r^2$=.28; $F$(1,105)=40.73, $p$\<.001, and revealed a significant positive relationship between the Stress and Symptoms, $b$=.008; $t$(105)=6.38, $p$\<.001."

### Reporting standardized $\beta$

That said, in many circles it's typical practice to report the *standardized* $beta$ coefficients ($\beta$). These are the $beta$ coefficients we would have obtained if we had initially converted both our *Stress* and *lnSymptom* values to $z$-scores, and then ran the model using those $z$-scores. For example, let's add z-transformed values for both to our original dataframe

```{r}
stress_data <- stress_data %>% mutate(z_Stress = scale(Stress)[,1],
                                      z_lnSymptoms = scale(lnSymptoms)[,1])
```

And now run the model (piping to a tidy output)

```{r}
z_model <- lm(z_lnSymptoms ~ z_Stress, data = stress_data)
broom::tidy(z_model)
```

That said, you don't **NEED** to do the z-transform by hand. We can retroactively obtain these coefficients from raw values by using the `lm.beta()` function from the `lm.beta` package (duplicate names, confusing I know). We can then input our model into the `lm.beta()` function and get the summary of those results. The new column **std_estimate** is reported in addition to the **estimate**:

```{r}
pacman::p_load(lm.beta)

# this is the raw data model you ran earlier
stress_symptoms_model <- lm(lnSymptoms ~ Stress, data = stress_data)
lm.beta(stress_symptoms_model) %>% broom::tidy()
```

Note that if you are using `summary` rather than `broom::tidy()` then the `lm.beta` output looks like this:

```{r}
lm.beta(stress_symptoms_model) %>% summary()
```

So amending the example reporting paragraph above:

> The resulting model was significant, $r^2$=.28; $F$(1,105)=40.73, $p$\<.001, and revealed a significant positive relationship between the Stress and Symptoms, $\beta =.528$; $t(105)=6.38$, $p<.001$."

Coincidentally you may have noticed that this $\beta$ value is the correlation that we established in the previous walkthrough! Quickly:

```{r}
cor(stress_data$Stress, stress_data$lnSymptoms)
```

As explained by Barry Cohen (in his useful textbook that I didn't have you buy...):

> if you have already tested the Pearson's $r$ against zero, there is no need to test the corresponding $b$, as the $t$ values will be exactly the same (except for rounding error) for both tests". More, a test on $b$ is equivalent to a test on $r$ in the one-predictor case. If it is true that $X$ and $Y$ are related, then it must also be true that $Y$ varies with $X$---that is, that the slope is nonzero. This suggests that a test on $b$ (beta coefficient) will produce the same answer as a test on $r$, and we could dispense with a test for $b$ altogether.

The reverse is also true. In the single predictor case, if one has a test of $b$ this obviates the need to run an independent test of $r$. The output of `lm` provides us with a simple test that gives us the significance of the beta coefficient, $b$.
