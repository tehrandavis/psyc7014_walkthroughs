---
title: "Within Subjects / Repeated Measures ANOVA"
html-math-method:
      method: mathjax
      url: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML"
---

## Within-Subject v. Between Subjects

Up until now we have considered ANOVA in between subjects designs; when data at each level of each factor is from a **different group** of participants. This week we more onto within-subjects designs. As we mentioned in class we have a within-subject design whenever data from the same participants exists on at least two levels of a factor (or analogously occupied at least two cells within our interaction matrix).

To contrast some important distinctions let's revisit our familiar data set contrasting test outcomes for students as a function of Lecture. I realize before, Lecture was crossed with at least one other factor, but for the sake on simplicity let's just consider data from this single factor. The goal of this first section is to contrast results as a function whether this data is considered within-subjects or between-subjects.

This walkthough assumes you have the following packages:

```{r}
pacman::p_load(tidyverse,
               rstatix,
               cowplot,
               performance,
               emmeans,
               afex,
               see)
```

Okay, not let's load in some data:

```{r}
within_between <- read_csv("http://tehrandav.is/courses/statistics/practice_datasets/within_between.csv")

within_between
```

### Within v Between ANOVA

So we have our dataset `within_between`. You'll note that there are two subjects columns `WithinSubjects` which imagines 12 participants each going through all 3 Lecture types and `BetweenSubjects` where each participant (N=36) is assigned to a single Lecture type. Previously, we might have treated this as a between subjects design. Looking at the `ezDesign` of this design we see that every `BetweenSubject` is assigned to a single condition (as evidenced by count = 1)

```{r}
ez::ezDesign(within_between,y=Lecture,x=BetweenSubjects_ID) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # rotate axis labels
```

Skipping past the preliminaries (e.g., testing for assumptions) and straight to running the BS ANOVA. Note that I'm using `lm()` here only to prove a point.

```{r}
between_aov <- lm(Score~Lecture, data = within_between)
rstatix::anova_test(between_aov, effect.size = "ges")
```

However, let's assume instead that this data set was collected using within design. That is, instead of different participants in each `Lecture` group, the same group of people went through all three lectures:

```{r}
ez::ezDesign(within_between,y=Lecture,x=WithinSubjects_ID)
```

We see that the `ezDesign` has changed. Instead of 36 participants each individually assigned to a single condition, we have 12 participants each assigned to all three conditions for a single trial (measure). As I briefly mentioned in class, conceptually, a **within subjects design considers participant as a pseudo factor** with each individual participant as a level. To fully capture this we should *factorize* our `WithinSubjects_ID` column.

Running the within-subjects ANOVA:

```{r echo=FALSE}
within_lm <- lm(Score~Lecture + factor(WithinSubjects_ID), data = within_between)
within_lm %>% rstatix::anova_test(effect.size = "pes")
```

Before continuing on, I want you to take a moment and think about what the specification of the formula in the `lm()` model above actually means. Not only are we using `Lecture` as a predictor, but we are also treating participant `WithinSubjects_ID` as a (psuedo) factor as well. That is not only is the model accounting for the variation moving between levels of `Lecture`, but also the model is accounting for variability moving between each participant. If you want to continue to use the `lm()` method for model building, you will need to include participant as a factor in the model. However, I would not recommend continuing with `lm()`, especially as you end up with more complex within-subjects designs. In your future, you will probably elect to run either a mixed effects model using `lmer` or a repeated measures ANOVA. Mixed effect modeling is outside of the scope of what we cover this semester (although we've laid some foundations for it).

As for repeated measures ANOVA, I would recommend turning to our new friend `afex`.

### Between v. Within ANOVA

Let's specify a between-subjects ANOVA model using the `afex` package. I personally prefer the formula method of `aov_car()`...

```{r}
# aov_car method
between_aov <- afex::aov_car(Score~Lecture + Error(BetweenSubjects_ID), data = within_between)
```

but in this walkthrough I'll be using the argument method of `aov_ez()`. Remember that BOTH do the same thing.

```{r}
# aov_ez method (alternative)
between_aov <- afex::aov_ez(
  id = "BetweenSubjects_ID",
  dv = "Score",
  data = within_between,
  between = "Lecture")
```

From here we can obtain a look at our ANOVA by calling `anova()` like so:

```{r}
anova(between_aov, es = "ges")
```

Things become a little more complex when we want to run a within-subjects ANOVA. We need to specify the error term in a slightly different way, by acknowledging that the error term is a function of the nesting between our independent variable and our participants (i.e., multiple levels of the IV are nested withing each subject). Using `aov_car` we specify the error term as `Error(WithinSubjects_ID/Lecture)`.

```{r}
within_aov <- afex::aov_car(Score~Lecture + Error(WithinSubjects_ID/Lecture), data = within_between)
```

Using `aov_ez` we simply list the variables under the `within` argument (well, we also need to change the `id` column, due to how I've constructed the demo set.

```{r}
# aov_ez method (alternative)
within_aov <- afex::aov_ez(
  id = "WithinSubjects_ID",
  dv = "Score",
  data = within_between,
  within = "Lecture")
```

We can also get a glimpse of our results using:

```{r}
anova(within_aov, es = "pes", correction = "none")
```

You may have noted that in the within case I added the argument `correction = "none"` to the `anova()` call. We'll talk more about that in a bit.

For now, remember that in both of these cases the data is exactly the same. What has changed is how we parse the variance (you'll notice that the denominator degrees of freedom are different for the second ANOVA). In a within design, we need to take into account the "within-subject" variance. That is how individual subjects vary from one level of treatment to the other. In this respect, within designs are typically more powerful than analogous between designs. While the inherent differences between individual subjects is present in both types of designs, your within-subjects ANOVA model includes it in its analysis. In the present example, this increase in power is reflected by the lower MSE (48.52 v. 139.36) and subsequently, larger F-value (12.31 v. 4.28) and effect size (0.53 v. 0.21) in our within-subjects analysis.

Well if that's the case why not run within-subject (WS) designs all of the time. Well, typically psychologists do when the subject lends itself to WS-designs. BUT there are certainly times when they are not practical, for example, if you are concerned about learning, practice, or carryover effects where exposure to a treatment on one level might impact the other levels---if you were studying radiation poisoning and had a placebo v. radiation dose condition, it be likely that you wouldn't run your experiment as a within---or at the very least you wouldn't give them the radiation first. It would also be likely that you'd be in violation several standards of ethics.

## a note on sphericity for Repeated measures ANOVA

In addition to the assumptions that we are familiar with, repeated-measures ANOVA has the additional assumption of Spherecity of Variance / Co-variance. We talk at length in class re: Spherecity of Variance / Co-variance so I suggest revisiting the slides for our example. At the same time there are debates as to the importance of sphericity in the subjects data. One alternative method that avoids these issues is to invoke mixed models (e.g., `lmer`). However, if you really want to go down the rabbit hole check out [Doug Bates reponse on appropriate dfs and p-values in `lmer`](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html). You'll note that these discussions were ten years ago and are still being debated (see [here](https://www.reddit.com/r/AskStatistics/comments/2x9ipp/linear_mixed_effect_models_and_pvalues_r/). That said, you may be seeing mixed models in your near future (i.e., next semester)

For now, we won't go down the rabbit hole and just focus on the practical issues confronted when running a repeated-measures ANOVA.

## EXAMPLE 1

To start, we will use data related to the effectiveness of relaxation therapy to the number of episodes that chronic migraine sufferers reported. Data was collected from each subject over the course of 5 weeks. After week 2 the therapy treatment was implemented effectively dividing the 5 weeks into two phases, Pre (Weeks 1 & 2) and Post (Weeks 3,4, & 5).

### loading in the data:

```{r}
example1 <- read_delim("https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab14-3.dat", delim = "\t")
example1
```

You'll notice that the data set above is in wide format. Each subject is on a single row and each week is in its own column. Note that **this is the preferred format for within subjects analysis for SPSS**. However in `R` we want it in long format.

```{r}
example1_long <- pivot_longer(example1,
                              cols = -Subject,
                              names_to = "Week", 
                              values_to = "Migraines")

example1_long$Week <- as.factor(example1_long$Week)
example1_long$Subject <- as.factor(example1_long$Subject)

example1_long
```

Ok, much better, each Subject × Week observation is on a single row.

### plotting the data

In addition to plotting your means, one crucial step is to plot the changes in the within-subject (repeated measures) variable at the participant-level. This is especially useful for discerning whether the pattern of results is roughly similar for all participants -OR- if, instead there is large individual variability in the direction of the effect. In other words, "is your manipulation having roughly the same effect for every participant OR is the effect drastically different for participants.

There are 2 ways to do this: 1. the Facet plot or 2. Spaghetti Plot.

### the Facet plot

Each window / facet is for a single subject (S1-S9):

```{r}
ggplot(data = example1_long, aes(x=Week, y=Migraines, group = 1)) +
  geom_point() + 
  geom_line() + 
  facet_wrap(~Subject, ncol = 3)
```

### the Spaghetti Plot

In this case each line represents an individual Subject (1-9):

```{r}
ggplot(data = example1_long, aes(x=Week, y=Migraines, group = Subject)) +
  geom_point(aes(col=Subject)) + 
  geom_line(aes(col=Subject))
```

Which you choose is ultimately up to you. I tend to use Spaghetti plots unless I have instances where I have a high number of participants. This can potentially make the spaghetti plot busy (a bunch of points and line). That said, the purpose of the Spaghetti Plot is to note inconsistencies in trends. A helpful tool for an overcrowded Spaghetti Plot is to use `plotly`. This produces an interactive plot where you can use the mouse cursor to identify important information:

```{r caption = "use your mouse on me... I'm interactive!!"}
pacman::p_load(plotly) # may need to install

s_plot <- ggplot(data = example1_long, aes(x=Week, y=Migraines, group = Subject)) +
  geom_point(aes(col=Subject)) + 
  geom_line(aes(col=Subject))

plotly::ggplotly(s_plot)
```

Try hovering over individual points and lines in the plot above. Also, see what happens when you click on one of the participant numbers in the legend.

For now this all the plotting we will do. Camera ready summary plots of repeated measures designs add a layer of complexity that warrants its own walkthrough.

### building the model using afex:

Running a WS ANOVA is just like running a BS ANOVA in `afex` (see the last walkthrough). We call in our within subjects factors using the `within=` argument.

```{r}
within_aov <- afex::aov_ez(id = "Subject", 
                           dv = "Migraines", 
                           data = example1_long,
                           between = NULL,
                           within = "Week"
                           )

# alternatively, we can use the formula interface of aov_car

# afex::aov_car(Migraines ~ Week + Error(Subject/Week), data = example1_long)
```

### assumptions checks: normality

We can then use the model, `within_aov` to run the requisite assumption checks. As before, the model plays nicely with `performance`, but the blanket `check_model` will not work with `afex`. Instead we can do a piecemeal check for normality along with visualizations. For some strange reason, when using `afex` we need to explicitly `print()` the output.

```{r}
within_aov %>% performance::check_normality() %>% print()
```

### assumptions checks: sphericity

Just like the paired $t$ test from a few weeks back, the RM-ANOVA does not have an explicit check for homogeneity of variance. Instead, RM ANOVA introduces a new test of assumptions that you must run, the **Mauchly Tests for Sphericity**. Indeed, this test is run **instead** of the check for homogeneity. This is because technically a WS ANOVA violates the assumption of independence of scores, and thus inflates the likelihood of a homogeneity of variances violation. Essentially, we give up on trying to hold onto the homogeneity assumption (resistance is futile) and instead direct our interests towards maintaining **sphericity**. Importantly, if we do violate sphericity, there is a clean and relatively agreed upon step that we must take with our analysis.

We can again use the `performance` library to check for sphericity (you can ignore the warning here):

```{r}
performance::check_sphericity(within_aov) %>% print()
```

In this case the results of Mauchly's test suggest we are ok (in this case $p > .05$). Now we can simply grab the output of our ANOVA, specifying`correction = "none"`.

```{r}
anova(within_aov, es = "pes", correction = "none")
```

That being said, there is an argument to be made that you should always make a correction to guard against deviations from sphericity. The correction becomes larger the further your data is from perfect sphericity (which can never truly be obtained in real data). However, standard practice in the psychology literature is to only apply the correction if our data fail the Mauchly's Test (*p* \< .05) [link](https://statistics.laerd.com/statistical-guides/sphericity-statistical-guide.php), so we will proceed on to the next steps.

### running post-hoc analyses and follow-ups

Post-hoc comparisons of means take a different form for repeated measures ANOVA. Typical methods such as Tukey HSD were designed for between-subjects effects, where it makes sense (assuming homogeneity of variance) to use a pooled error term. **However, mutl**

The typical recommendation (more detail on this in the following walkthrough) is to carry out pair-wise contrasts for a within-subjects factor using ordinary paired tests with an error term based only on the levels being compared.

Let's briefly think back to when we were running post-hoc (and simple effects) analyses on Between Subject ANOVA. A point was made that pairwise comparisons should be considered in light of the full model, meaning that the tests needed to consider the Error terms, including **df** from the omnibus ANOVA. Simply put, the recommendation above is saying that you should **NOT** do the same when running a repeated measures ANOVA.

```{r}
emm_Week <- emmeans(within_aov, specs = ~Week)
emm_Week %>% pairs()
```

Note that the `df` (df = 8) of these tests are **NOT** the error df (df = 22) from the omnibus ANOVA. Also note that the `SE` of each comparison is different.

### Example 1 write up

> To test whether relaxation therapy had a positive effect for migraine suffers, we analyzed the number of migraines each individual reported over a 5 week period. The first two weeks were used as a baseline to establish a typical week for each person. The remaining three weeks each person was lead through relaxation therapy techniques. These data were submitted to a within-subjects ANOVA with Week as a factor.
>
> Post hoc comparisions demonstrated the effectiveness of our treatment. While the number of migraines remained indifferent during the first two weeks (TukeyHSD, *p* \>.05) there was a significant decrease in the number of migraines post treatment—the number of migraines in Weeks 3, 4, 5 we significantly lower than Weeks 1 and 2 (*p*s \< .05).

\*\* note that you don't have to worry about the sphericity violation as it relates to the pairwise post hoc analyses---as you are only comparing two levels.

## EXAMPLE 2

Let's take a look at another example, using a experimental paradigm we are familiar with, scores as a function of lecture type.

### loading in the data

You note that this data is already in long format so no need to adjust.

```{r}
example2_long <- read_csv("http://tehrandav.is/courses/statistics/practice_datasets/withinEx2.csv")

example2_long
```

### plotting the data

Why don't you try to create a Spaghetti plot!!!

### running the ANOVA:

As before, let's run this using afex:

```{r}
within_aov <- afex::aov_ez(
  id = "Subject", 
  dv = "Score", 
  data = example2_long,
  between = NULL,
  within = "Lecture"
)

# alternative with aov_car:

# within_aov <- 
#   afex::aov_car(Score ~ Lecture + Error (Subject/Lecture), 
#                 data = example2_long)
```

```{r}
performance::check_sphericity(within_aov)
```

You'll notice that in this example the data failed the Mauchly Test for Sphericity ($p$ = .012). In this case you'll need to make the appropriate corrections. To do so you have two possibile choices, the **Greenhouse-Geisser** method or the **Huynh-Feldt** method.

The Greenhouse-Geisser (GG) correction is more conservative and is generally recommended when the epsilon value from Mauchly's Test is less than .75. It adjusts the degrees of freedom used in significance testing, thereby increasing the robustness of the results against violations of sphericity.

The Huynh-Feldt (HF) correction is less conservative and may be more appropriate when the epsilon is greater than .75, suggesting a lesser degree of sphericity violation. It also modifies the degrees of freedom but to a lesser extent than GG, resulting in a less stringent correction.

GG corrections are the "industry standard" (you typically see these in psych literature). Let's go ahead an do this now and return to HF methods at the end of the walk through.

```{r}
anova(within_aov, es = "pes", correction = "GG")
```

### post-hocs and follow-ups

```{r}
emmeans(within_aov, 
        spec= ~Lecture) %>% 
  pairs(adjust="tukey")
```

### Example 2 write up

Again I need to get my summary stats for the write up. Since I'm not doing any custom contrasts I can just pull the cell values from my summary table:

```{r}
example2_long %>%
  group_by(Lecture) %>%
  rstatix::get_summary_stats(Score, type ="mean_se")
```

And now for the write-up:

... to test this hypothesis we ran a within subjects ANOVA. Due to a violation of the sphericity assumption, we used Greenhouse-Geisser corrected degress of freedom. Our analysis revealed a significant effect for Lecture, $F$(1.26, 13.88) = 12.31, $p$ = .002, $\eta_p^2$ = .53. Post-hoc analyses revealed participants scores in the Physical condition ($M$ ± $SE$: 40.0 ± 3.12) was significantly greater than both History (34.5 ± 2.4) and Social (26.0 ± 4.4) scores ($p$s \< .05). History and Social were not different from one another.

Take a look at the next walkthrough for an example of factorial repeated measures ANOVA.

## a note on using the Huynh-Feldt correction and how these funky $df$ are calculated.

As mentioned above, GG corrections are the "standard" and typically show up in psychology literature. However, if one is willing to be less conservate (i.e., accept more risk of Type 1 error) then they may elect to use HF corrections assuming the criteria have been met. Remembner that HF is less conservative and only really appropriate when the epsilon is greater than .75 (i.e., there is a lesser degree of spherecity violation). Unfortunately `performance::check_spherecity()` doesn't give us an output of epsilon, only the resulting `p_value`. Using the model from example 2:

```{r}
performance::check_sphericity(within_aov) %>% print()
```

Fortunately, `afex` has it's own baked in test that can be used instead of `performance`. That said, it involves print out a ton of results

```{r}
summary(within_aov)
```

The output above can be broken down into three parts:

-   the top contains the actual output of the ANOVA minus the corrections.
-   the second section is the Mauchly test.
-   the third part includes the epsilons (eps) for both the GG and HF tests, as well as whether the resulting model is significant.

In this case the `HF eps` (0.675) is not \> 0.75 so we should use the GG correction. If we were doing this by hand, this would involve taking the `GG eps,` $\epsilon$ , and multiplying it by our predictors original $df$ to get the corrected $df$.

$$
\text{corrected num }df = \text{original numerator } df \; \times \; \epsilon \newline
\text{corrected den }df = \text{original denominator } df \; \times \; \epsilon
$$

In this case that is `2 * 0.63101` ≈ 1.26 and `22 * 0.63101` ≈ 13.88. You'll note that these are the $df$ from:

```{r}
anova(within_aov, es = "pes", correction = "GG")
```

If instead we decided we wanted to take more risk and use HF, we would perform the exact sampe process only using `HF eps`. To impliment this in our `anova()` we simply change the call to `correction = "HF".`

```{r}
anova(within_aov, es = "pes", correction = "HF")
```

You notice that the resulting $df$s are different.

OK. Let's move onto the next walkthrough where we look at factorial repeated measures ANOVA.